---
title: "data_cleaning_for_noobs"
author: "gg"
date: "11/19/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Warning : built over one dataset. Surely other datasets will pose many problems but it's the beginning of a solution here.

## Data Cleaning

#### Libraries import
```{r}
library(tidyverse)
library(magrittr)
library(readr)
library(stringr)

Sys.setlocale(locale="en_US.UTF-8")
```

#### Read data and primary data cleaning

Read the data (in the future, we will have to rbind all the scrapped listings - a function should do):

```{r}
mapping_villes <- data.frame(read_csv2("datasets/mapping_villes.csv"))

data <- read_csv("datasets/scrapped_listings_1_1_500.csv")

#todo:

#function that reads and rowbinds the list of csv

#assign all to data
```

Remove the deleted listings, and the useless column that is X1:

```{r}
data <- data %>% 
  filter(is_listing_deleted == 0) %>% 
  select(-X1, -is_listing_deleted)

data <- data.frame(data)
```

```{r}
View(data)
```

#### URL

We keep URL.

#### Brand

```{r}

# To check over other datasets, but for now it seems occasion is mentioned every time, so let's just remove it

sum(sapply(data$brand, function(x) str_detect(x, "occasion"))) / length(data$brand)

# Actual cleaning

data$brand <- sapply(data$brand, function(x) str_remove(x, " occasion")[1], USE.NAMES = F)

# Check

data %>% 
  distinct(brand)

# Good, we reduced the brand column to their actual brand names
```

#### Modèle

For now, let's just consider that the modèle is an additional information but not a category, as in one we would use in our filtering or models.

#### Adresse pro / particulier

For now, we will focus on retrieving the postal code from the adress and the city from mapping_villes.csv (source : http://www.nosdonnees.fr/wiki/index.php/Fichier:EUCircos_Regions_departements_circonscriptions_communes_gps.csv.gz)

We'll use a regular expression to find the postal code : "(([0-8][0-9])|(9[0-5])|(2[ab]))[0-9]{3}"


```{r}
# Combine adress particulier and pro : coalesce takes the first value, but if null, imputes the second value

data$all_address <- coalesce(data$address_particulier, data$address_pro)

get_postal_code <- function(address) {
  # Get all the strings that match a possible postal code
  poss_postal_codes <- str_match_all(address, "(([0-8][0-9])|(9[0-5])|(2[ab]))[0-9]{3}")[[1]][,1]
  l <- length(poss_postal_codes)
  # If there isn't any, return NA
  if (l == 0) {
    NA
  }
  # If there is just one, good, we keep that one
  else if (l == 1) {
     poss_postal_codes[1]
  } 
  # If there are many, we test them again, considering that scenario : we have both a postal box (BP) and a postal code. We want the one that is most likely to be a postal code (should usually end with a 0), and keep that one.
  # Note: we don't use that usually. Some postal codes end don't end with 0, ex in Paris : 75001.
  else {
    better_postal_code <- str_match(poss_postal_codes, "(([0-8][0-9])|(9[0-5])|(2[ab]))[0-9]{2}[0]")[,1]
    better_postal_code[!is.na(better_postal_code)]
    }
}

data$postal_codes <- sapply(data$all_address, get_postal_code, USE.NAMES = F)
```

#### Join on long, lat and city

It's not the most efficient thing to do codewise, surely, but it does ensure that we'll have the same format for everything.

```{r}
# Filter the information we're interested in
relevant_mapping_villes <- mapping_villes %>% 
  select(code_région, nom_région, numéro_département, nom_département, nom_commune, codes_postaux, latitude, longitude)

# Join with postal codes
data <- data %>% 
  left_join(relevant_mapping_villes, by= c("postal_codes" = "codes_postaux"))
```

#### Name pro + is pro

Name pro will be useful only when the seller is a pro. Let's just keep it that way and create an "is_pro" binary column for easy access to this information.

```{r}
data$is_pro <- (1 - is.na(data$name_pro))
```

#### Année
