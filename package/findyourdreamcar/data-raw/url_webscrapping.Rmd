---
title: "url_webscrapping"
author: "Arnaud Fournier"
date: "22/10/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
```{r prerequisites, message=FALSE, warning=FALSE}
# Let's prepare our environment
library(rvest)
library(magrittr)
library(dplyr)
Sys.setlocale("LC_CTYPE", "en_US.UTF-8") # To see the currencies
```





Objective: Get a list of URL of all the individual listings on paru vendu

1) Get the list of url on which listings are
2) Scrap each of this page to get the listings url



```{r List of URL to scrap, echo=FALSE}
listingpageURL <- paste0( "https://www.paruvendu.fr/auto-moto/listefo/default/default?auto-typeRech=&reaf=1&r2=&px1=&md=&codeINSEE=&lo=&ray=15&r=VVO00000&r1=&trub=&nrj=&pa=&km1=&npo=0&a0=&a1=&fulltext=&tr=&pf0=&pf1=&p=", c(1:1))
```




#### B. Downloading the webpage

```{r Downloading webpage}
# Specifying the url of the listing to be scrapped
url <- "https://www.paruvendu.fr/auto-moto/listefo/default/default?auto-typeRech=&reaf=1&r2=&px1=&md=&codeINSEE=&lo=&ray=15&r=VVO00000&r1=&trub=&nrj=&pa=&km1=&npo=0&a0=&a1=&fulltext=&tr=&pf0=&pf1=&p=1"
# Reading the HTML code from the website
webpage <- read_html(url)
```




```{r Finding paths}
# Defining the xpath of every element we want to extract
xpath_url <- "//*[@class='ergov3-voirann']"
```



```{r Extracting Raw Data}


URLtest <- webpage %>% 
  html_nodes(xpath = xpath_url) %>% 
  html_nodes("a") %>% 
  html_attr("href") 


URLtest <- URLtest[which(regexpr('https',URLtest) >= 1)] #Get all elements from the node that contains a url - pattern is https
URLtest <- URLtest[-which(regexpr('pubcommunfo',URLtest) >= 1)] #Take out all elements that are adverts


URLtest
```

#### C. Automating the process

```{r function for scrapping paru-vendu, eval=FALSE}
#Creating a function for Paru-Vendu that scraps the URL of all individual listing

listingurlscrapper <- function(url){
  webpage <- read_html(url) #Get url of the listing page and extract html code
  xpath_url <- "//*[@class='ergov3-voirann']" #Listings URL are in nodes of class 'ergov3-voirann'
  
urllist <- webpage %>%  #Get a list URL
  html_nodes(xpath = xpath_url) %>% 
  html_nodes("a") %>%  #URL are in nodes a in the div of class 'ergov3-voirann'
  html_attr("href")  #We take out only attributes of form "href"
  
urllist <- urllist[which(regexpr('https',urllist) >= 1)] #Get all elements from the node that contains a url - pattern is https
urllist <- urllist[-which(regexpr('pubcommunfo',urllist) >= 1)] #Take out all elements that are adverts
Sys.sleep(sample(10,1)*0.1)
return(urllist)
}




# Lets make a test of what woudl happend if one of the url we were to scrap wasnt right
listingpageURL[3] <- "https://www.paruvendu.fr/auto-moto/listefo/default/default?auto-typeRech=&reaf=1&r2=&px1=&md=&codeINSEE=&lo=&ray=15&r=VVO00000&r1=&trub=&nrj=&pa=1=&npo=0&a0=&a1=&fulltext=&tr=&pf0=&pf1=&p=3"
#Lets run the function
urldata <- lapply( listingpageURL, listingurlscrapper) %>% 
  unlist() %>%
  
  #data.frame() %>%  Doesnt work if you make it a dataframe because it makes the object a factor - doesn't work with lapply (read_html function doesnt take factors)
  distinct()
#It works although one of the url isnt working
#So we can have peace of mind when scrapping url
urldata[1:7]
class(urldata)
```





